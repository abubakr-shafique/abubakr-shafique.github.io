<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Abubakr Shafique, PhD</title>

    <meta name="author" content="Abubakr Shafique">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Abubakr Shafique, PhD
                </p>
                <p>
				I am a <a href="https://www.torontomu.ca/graduate/postdoctoral-fellows/torontomet-postdoctoral-fellows/postdoc-bios/abubakr-shafique/">Post-Doctoral Researcher</a> at <a href="https://www.torontomu.ca/akhademi/">Image Analysis in Medicine Lab (IAMLAB)</a>, Toronto Metropolitan University, Toronto, ON, Canada. <br>
				<br>
				I am a results-driven scientist with a Ph.D. from the University of Waterloo, Canada, specializing in Artificial Intelligence (AI) and Machine Learning (ML) with a focus on Medical Image Analysis and Representation Learning.<br>
				My expertise spans the development and implementation of advanced algorithms for computational biology, integrating AI innovation with biomedical applications. 
				Combining strong academic foundations with hands-on research experience, I excel at tackling complex interdisciplinary problems in AI and medicine, driving discoveries that bridge data science and clinical impact.
                </p>
                <p style="text-align:center">
                  <a href="mailto:abubakr.shafique@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Abubakr_Shafique-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/AbubakrShafique-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=zlnaTcIAAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                  <!--<a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                  <a href="https://github.com/abubakr-shafique">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/AB.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/AB.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2><b>Research:</b></h2>
                <p>
                  I am interested in computer vision, deep learning, generative AI, image processing, and natural language processing.<br>
				  My research primarily focuses on representation learning and understanding self-supervised learning and foundation models in medical imaging, encompassing both uni-modal (vision or language) and multi-modal (vision-language, vision-genomics) frameworks.<br> 
				  <br>
				  <b>Some papers are:</b>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>      
  

            <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PathDino.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
<!--                 <img src='images/PathDino.jpg' width="160"> -->
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('PathDino').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('PathDino').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Alfasly_Rotation-Agnostic_Image_Representation_Learning_for_Digital_Pathology_CVPR_2024_paper.html">
                <papertitle><b>Rotation-Agnostic Image Representation Learning for Digital Pathology</b></papertitle>
              </a>
              <br>
	      <a>Saghir Alfasly</a>,
              <strong>Abubakr Shafique</strong>,
              <a >Peyman Nejat</a>, 
		<a> Jibran Khan </a>
	      <a >Areej Alsaafin</a>, 
	      <a >Ghazal Alabtah</a>, 
	      <a >H.R.Tizhoosh</a>, 
              <br>
              <em>CVPR</em>, 2024
              <br>
	      [<a href="https://kimialabmayo.github.io/PathDino-Page/">Project webpage</a>]
              [<a href="https://huggingface.co/spaces/Saghir/PathDino">Demo</a>]
              [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Alfasly_Rotation-Agnostic_Image_Representation_Learning_for_Digital_Pathology_CVPR_2024_paper.html">Paper</a>]
	      [<a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Alfasly_Rotation-Agnostic_Image_Representation_CVPR_2024_supplemental.pdf">Supplementary</a>]
		[<a href="https://github.com/KimiaLabMayo/PathDino">Code</a>]
		[<a href="https://portal.gdc.cancer.gov/repository?facetTab=files&filters=%7B%22op%22%3A%22and%22%2C%22content%22%3A%5B%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22cases.project.program.name%22%2C%22value%22%3A%5B%22TCGA%22%5D%7D%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.access%22%2C%22value%22%3A%5B%22open%22%5D%7D%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.data_format%22%2C%22value%22%3A%5B%22svs%22%5D%7D%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.experimental_strategy%22%2C%22value%22%3A%5B%22Diagnostic%20Slide%22%5D%7D%7D%5D%7D">Data</a>]
              <p></p>
              <p>
              In this work, we introduce a fast patch selection method (FPS) for efficient selection of representative patches while preserving spatial distribution. HistoRotate, is a 360∘ rotation augmentation for training histopathology models, enhancing learning without compromising contextual information. PathDino, is a compact histopathology Transformer with five small vision transformer blocks and ≈9 million parameters.
              </p>
            </td>
          </tr>

		 		 	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/FM.jpg' width="160"></div>
                <img src='images/FM.jpg' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('FM').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('FM').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S2949761224000142">
                <papertitle><b>Foundation Models for Histopathology — Fanfare or Flair</b></papertitle>
              </a>
              <br>
	      <a>Saghir Alfasly</a>,
	      <a >Peyman Nejat</a>, 
		   <a>   Sobhan Hemati</a>,
		   <a>   Jibran Khan</a>,
		   <a>   Isaiah Lahr</a>,
		   <a>   Areej Alsaafin</a>,
		   <strong>   Abubakr Shafique</strong>,
		   <a>   Nneka Comfere</a>,
		   <a>   Dennis Murphree</a>,
		   <a>   Chady Meroueh</a>,
		   <a>   Saba Yasir</a>,
		  <a>    Aaron Mangold</a>,
		 <a>     Lisa Boardman</a>,
		  <a>   Vijay H. Shah</a>,
		     <a> Joaquin J. Garcia</a>,
		    <a>  H.R. Tizhoosh</a>,
              <br>
              <em>Mayo Clinic Proceedings: Digital Health</em>, 2024
              <br>
              [<a href="https://www.sciencedirect.com/science/article/pii/S2949761224000142">Paper</a>]
	      [<a href="https://ars.els-cdn.com/content/image/1-s2.0-S2949761224000142-mmc1.pdf">Supplementary</a>]
              <p></p>
              <p>
              This paper investigates the efficacy of the foundation models in the domain of histopathology by conducting a detailed comparison between these models, specifically CLIP derivatives (PLIP and BiomedCLIP), and traditional, domain-specific histology models that leverage well-curated datasets. Through a rigorous evaluation process on eight diverse datasets, including four internal from Mayo Clinic and four well-known public datasets (PANDA, BRACS, CAMELYON16, DigestPath). The findings show that domain-specific models, such as DinoSSLPath and KimiaNet, provide better performance across various metrics, underlining the significance of clean large datasets for histopathological analyses. 
              </p>
            </td>
          </tr>	
		 
		 	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/SDM.gif' width="160"></div>
                <img src='images/SDM.gif' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('SDM').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('SDM').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.09902">
                <papertitle><b>Selection of Distinct Morphologies to Divide & Conquer Gigapixel Pathology Images</b></papertitle>
              </a>
              <br>
              <strong>Abubakr Shafique</strong>,
              <a>Saghir Alfasly</a>,
	      <a >Areej Alsaafin</a>,
              <a >Peyman Nejat</a>, 
	      <a> Jibran Khan </a>
	      <a >H.R.Tizhoosh</a>,
              <br>
              <em>Preprint</em>, 2023
              <br>
              [<a href="https://arxiv.org/abs/2311.09902">Paper</a>]
	      [<a href="https://arxiv.org/pdf/2311.09902.pdf">Supplementary</a>]
              <p></p>
              <p>
              We propose SDM, a novel method for selecting diverse WSI patches, minimizing patch count while capturing all morphological variations. SDM outperforms the state-of-the-art, achieving high representativeness without needing parameter tuning.
              </p>
            </td>
          </tr>	

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		   
            <tr>
              <td>
                <h2><b>Certifications:</b></h2>
              </td>
            </tr>
			
            <tr>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <a href="https://www.coursera.org/account/accomplishments/specialization/VR14XQN1IG7M"> <b>Cancer Biology</b>: The Johns Hopkins University (October 2025) </a>
                <br>
                <a href="https://www.coursera.org/account/accomplishments/verify/9N2ZYWKPPDD9"> <b>Introduction to the Biology of Cancer</b>: The Johns Hopkins University (October 2025) </a>
                <br>
                <a href="https://www.coursera.org/account/accomplishments/verify/TWQ9P72W3C5P"> <b>Understanding Cancer Metastasis</b>: The Johns Hopkins University (October 2025) </a>
                <br>
                <a href="https://www.coursera.org/account/accomplishments/verify/JNP5BNJ5EEV7"> <b>Understanding Prostate Cancer</b>: The Johns Hopkins University (October 2025) </a>
				<br>
                <a href="https://www.coursera.org/account/accomplishments/verify/4YE5HKV6YUIJ"> <b>Data Science in Stratified Healthcare and Precision Medicine</b>: The University of Edinburgh (October 2025) </a>
				<br>
                <a href="https://www.coursera.org/account/accomplishments/verify/1XJUPYNJQNU6"> <b>Introduction to Neurohacking In R</b>: The Johns Hopkins University (October 2025) </a>
				<br>
                <a href="https://www.coursera.org/account/accomplishments/verify/QX6YWBQJB7LC"> <b>An Introduction to Practical Deep Learning</b>: Intel Corporation (December 2020) </a>
				<br>
                <a href="https://www.coursera.org/account/accomplishments/verify/D8CTL9YS76DE"> <b>Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning</b>: DeepLearning.AI (August 2020) </a>
				<br>
                <a href="https://www.coursera.org/account/accomplishments/verify/AUPWNCZHB7C8"> <b>Neural Networks and Deep Learning</b>: DeepLearning.AI (August 2020) </a>
				<br>
                <a href="https://www.coursera.org/account/accomplishments/verify/LP5N4UEXDD9Q"> <b>COVID-19 Contact Tracing</b>: The Johns Hopkins University (July 2025) </a>
				<br>
              </td>
            </tr>
		   
            <tr>
              <td>
                <h2><b>Invited Talks:</b></h2>
              </td>
            </tr>

            <tr>
              <td style="padding:0px;width:100%;vertical-align:center">
                <a href=""> <b>How Fair are Foundation Models? Exploring the Role of Covariate Bias in Histopathology</b>, at Stanford MedAI Group <br>(October, 2025)</a><br>				  
                <a href=""> <b>How Fair are Foundation Models? Exploring the Role of Covariate Bias in Histopathology</b>, at Abbvie CVRT Imaging Seminar <br>(February, 2026)</a><br>
              </td>
            </tr>
			
			
			<tr>
              <td>
                <h2><b>Academic Services:</b></h2>
              </td>
            </tr>
			
            <tr>
              <td style="padding:0px;width:100%;vertical-align:center">
                <a href="https://link.springer.com/journal/10278">Academic Reviewer: Journal of Imaging Informatics in Medicine (2025)</a><br>
              </td>
            </tr>
		
			
			<tr>
              <td>
                <h2><b>Teaching:</b></h2>
              </td>
            </tr>			
           
            <tr>
              <td style="padding:0px;width:100%;vertical-align:center">
                <a href="">Graduate Teaching Assistant: Department of Engineering, University of Waterloo.</a>
                <br>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Abubakr Shafique, PhD
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
